// Open Terminal
$ su hduser
$ cd /usr/local/hadoop/hadoop-2.7.2/sbin
$ start-dfs.sh
$ start-yarn.sh
$ jps
// Open New Terminal
$ cd Desktop/
$ mkdir inputdata
$ cd inputdata/
$ echo “Hai, Hello, How are you? How is your health?” >> hello.txt
$ cat >> hello.txt
// Go back to old Terminal
$ hadoop fs –copyFromLocal /home/hduser/Desktop/inputdata/hello.txt /folder/hduser
/usr/local/hadoop/hadoop-2.7.2/share/hadoop/commonand
/usr/local/hadoop/hadoop-2.7.2/share/hadoop/mapreduce folders.
// WordCount.java
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.io.Text;
public class WordCount extends Configured implements Tool {
@Override
public int run(String[] arg0) throws Exception {
if(arg0.length<2)
{
System.out.println("check the command line arguments");
}
JobConf conf=new JobConf(WordCount.class);
FileInputFormat.setInputPaths(conf, new Path(arg0[0]));
FileOutputFormat.setOutputPath(conf, new Path(arg0[1]));
conf.setMapperClass(WordMapper.class);
conf.setReducerClass(WordReducer.class);
conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(IntWritable.class);
conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(IntWritable.class);
JobClient.runJob(conf);
return 0;
}
public static void main(String args[]) throws Exception
{
int exitcode=ToolRunner.run(new WordCount(), args);
System.exit(exitcode);
}
}
// WordCountMapper.java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.Mapper;
public class WordCountMapper extends MapReduceBase implements
Mapper<LongWritable,Text,Text,IntWritable>
{
@Override
public void map(LongWritable arg0, Text arg1, OutputCollector<Text, IntWritable> arg2,
Reporter arg3)
throws IOException {
String s=arg1.toString();
for(String word:s.split(" "))
{
arg2.collect(new Text(word),new IntWritable(1));
}
}
}
// WordCountReducer.java
import java.io.IOException;
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.io.Text;
public class WordCountReducer implements Reducer<Text,IntWritable,Text,IntWritable> {
@Override
public void configure(JobConf arg0) {
}
@Override
public void close() throws IOException {
}
@Override
public void reduce(Text arg0, Iterator<IntWritable> arg1, OutputCollector<Text, IntWritable>
arg2, Reporter arg3)
throws IOException {
int count=0;
while(arg1.hasNext())
{
IntWritable i=arg1.next();
count+=i.get();
}
arg2.collect(arg0,new IntWritable(count));
}
}
// To view results in old Terminal
$hdfs dfs -cat /usr/local/hadoop/output/part-r-00000
// To Remove folders created using hdfs
$ hdfs dfs -rm -R /usr/local/hadoop/output
